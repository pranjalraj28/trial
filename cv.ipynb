{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranjalraj28/trial/blob/main/cv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULZHVR63l11M",
        "outputId": "40cce1c9-02db-4ba3-9d20-f761218e68a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['together', 'dog', 'sat', 'and', 'cat', 'mat', 'played', 'on', 'the', 'log']\n",
            "TF-IDF Matrix:\n",
            " [[0.         0.         0.         0.         0.         0.08109302\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.08109302]\n",
            " [0.08109302 0.         0.         0.08109302 0.         0.\n",
            "  0.08109302 0.         0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "#1\n",
        "import numpy as np\n",
        "\n",
        "def compute_tf_idf(docs, vocab):\n",
        "    n_docs = len(docs)\n",
        "    n_vocab = len(vocab)\n",
        "    tf = np.zeros((n_docs, n_vocab))\n",
        "\n",
        "    # Build term frequency matrix\n",
        "    for i, doc in enumerate(docs):\n",
        "        words = doc.lower().split()\n",
        "        for word in words:\n",
        "            if word in vocab:\n",
        "                j = vocab.index(word)\n",
        "                tf[i, j] += 1\n",
        "        tf[i] /= len(words)  # Normalize TF\n",
        "\n",
        "    # Compute Document Frequency (DF)\n",
        "    df = np.zeros(n_vocab)\n",
        "    for j, term in enumerate(vocab):\n",
        "        df[j] = sum(1 for doc in docs if term in doc.lower().split())\n",
        "\n",
        "    # Compute Inverse Document Frequency (IDF)\n",
        "    idf = np.log(n_docs / (df + 1))  # Add 1 to avoid division by zero\n",
        "\n",
        "    # Compute TF-IDF matrix\n",
        "    tf_idf = tf * idf  # Element-wise multiplication\n",
        "\n",
        "    return tf_idf\n",
        "\n",
        "# Example usage:\n",
        "documents = [\n",
        "    \"cat sat on the mat\",\n",
        "    \"dog sat on the log\",\n",
        "    \"cat and dog played together\"\n",
        "]\n",
        "\n",
        "vocabulary = list(set(\" \".join(documents).lower().split()))\n",
        "tf_idf_matrix = compute_tf_idf(documents, vocabulary)\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"TF-IDF Matrix:\\n\", tf_idf_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "def generate_ngrams(sentence, n):\n",
        "    words = sentence.lower().split()\n",
        "    ngrams = []\n",
        "    for i in range(len(words) - n + 1):\n",
        "        ngram = words[i:i + n]\n",
        "        ngrams.append(ngram)\n",
        "    return ngrams\n",
        "\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "n = 3\n",
        "ngrams = generate_ngrams(sentence, n)\n",
        "print(f\"{n}-grams:\")\n",
        "for gram in ngrams:\n",
        "    print(gram)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNjZhrzZl2rV",
        "outputId": "0ce8a959-6a7a-43ce-d136-7a1e959f7382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3-grams:\n",
            "['the', 'quick', 'brown']\n",
            "['quick', 'brown', 'fox']\n",
            "['brown', 'fox', 'jumps']\n",
            "['fox', 'jumps', 'over']\n",
            "['jumps', 'over', 'the']\n",
            "['over', 'the', 'lazy']\n",
            "['the', 'lazy', 'dog.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "from collections import Counter\n",
        "def compute_trigram_language_model(documents):\n",
        "    all_trigrams = []\n",
        "\n",
        "    # Generate trigrams from each document\n",
        "    for doc in documents:\n",
        "        words = doc.lower().split()\n",
        "        for i in range(len(words) - 2):\n",
        "            trigram = tuple(words[i:i + 3])  # Create a 3-word tuple\n",
        "            all_trigrams.append(trigram)\n",
        "\n",
        "    # Count the frequency of each trigram\n",
        "    trigram_counts = Counter(all_trigrams)\n",
        "    total_trigrams = sum(trigram_counts.values())\n",
        "\n",
        "    # Compute probabilities for each trigram\n",
        "    trigram_probabilities = {}\n",
        "    for trigram, count in trigram_counts.items():\n",
        "        trigram_probabilities[trigram] = count / total_trigrams\n",
        "\n",
        "    return trigram_probabilities\n",
        "\n",
        "\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"The quick blue fox jumps over the lazy cat\",\n",
        "    \"The lazy dog sleeps under the blue sky\"\n",
        "]\n",
        "\n",
        "trigram_model = compute_trigram_language_model(documents)\n",
        "\n",
        "print(\"Trigram Probabilities:\")\n",
        "for trigram, prob in trigram_model.items():\n",
        "    print(f\"{trigram}: {prob:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvRXPavdmHi7",
        "outputId": "f49247f8-3d44-40ff-ad48-b564b3631bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trigram Probabilities:\n",
            "('the', 'quick', 'brown'): 0.0500\n",
            "('quick', 'brown', 'fox'): 0.0500\n",
            "('brown', 'fox', 'jumps'): 0.0500\n",
            "('fox', 'jumps', 'over'): 0.1000\n",
            "('jumps', 'over', 'the'): 0.1000\n",
            "('over', 'the', 'lazy'): 0.1000\n",
            "('the', 'lazy', 'dog'): 0.1000\n",
            "('the', 'quick', 'blue'): 0.0500\n",
            "('quick', 'blue', 'fox'): 0.0500\n",
            "('blue', 'fox', 'jumps'): 0.0500\n",
            "('the', 'lazy', 'cat'): 0.0500\n",
            "('lazy', 'dog', 'sleeps'): 0.0500\n",
            "('dog', 'sleeps', 'under'): 0.0500\n",
            "('sleeps', 'under', 'the'): 0.0500\n",
            "('under', 'the', 'blue'): 0.0500\n",
            "('the', 'blue', 'sky'): 0.0500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "import numpy as np\n",
        "\n",
        "def create_embedding_matrix(corpus, embedding_dim):\n",
        "    # Preprocessing\n",
        "    vocabulary = {}\n",
        "    index = 0\n",
        "    for sentence in corpus:\n",
        "        words = sentence.lower().split()\n",
        "        for word in words:\n",
        "            if word not in vocabulary:\n",
        "                vocabulary[word] = index\n",
        "                index += 1\n",
        "\n",
        "    V = len(vocabulary)\n",
        "    # Initialize embedding matrix with random values between 0 and 1\n",
        "    E = np.random.rand(V, embedding_dim)\n",
        "\n",
        "    def get_word_vector(word):\n",
        "        word = word.lower()\n",
        "        if word in vocabulary:\n",
        "            idx = vocabulary[word]\n",
        "            return E[idx]\n",
        "        else:\n",
        "            return np.zeros(embedding_dim)\n",
        "\n",
        "    return E, vocabulary, get_word_vector\n",
        "\n",
        "# Example usage:\n",
        "corpus = [\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning is amazing\",\n",
        "    \"I love learning new things\"\n",
        "]\n",
        "embedding_dim = 3\n",
        "\n",
        "E, vocabulary, get_word_vector = create_embedding_matrix(corpus, embedding_dim)\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"Embedding Matrix E:\\n\", E)\n",
        "\n",
        "# Test get_word_vector\n",
        "word = \"learning\"\n",
        "vector = get_word_vector(word)\n",
        "print(f\"Embedding for '{word}':\", vector)\n",
        "\n",
        "# Test with a word not in the vocabulary\n",
        "word = \"unknown\"\n",
        "vector = get_word_vector(word)\n",
        "print(f\"Embedding for '{word}':\", vector)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv3vVK1Um4jy",
        "outputId": "12f3b90e-dfab-4123-92bd-31e54f86c81b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'i': 0, 'love': 1, 'machine': 2, 'learning': 3, 'is': 4, 'amazing': 5, 'new': 6, 'things': 7}\n",
            "Embedding Matrix E:\n",
            " [[0.31856895 0.66741038 0.13179786]\n",
            " [0.7163272  0.28940609 0.18319136]\n",
            " [0.58651293 0.02010755 0.82894003]\n",
            " [0.00469548 0.67781654 0.27000797]\n",
            " [0.73519402 0.96218855 0.24875314]\n",
            " [0.57615733 0.59204193 0.57225191]\n",
            " [0.22308163 0.95274901 0.44712538]\n",
            " [0.84640867 0.69947928 0.29743695]]\n",
            "Embedding for 'learning': [0.00469548 0.67781654 0.27000797]\n",
            "Embedding for 'unknown': [0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "import numpy as np\n",
        "\n",
        "def create_embedding_matrix_with_pretrained(corpus, pretrained_embeddings, embedding_dim):\n",
        "    # Preprocessing\n",
        "    vocabulary = {}\n",
        "    index = 0\n",
        "    for sentence in corpus:\n",
        "        words = sentence.lower().split()\n",
        "        for word in words:\n",
        "            if word not in vocabulary:\n",
        "                vocabulary[word] = index\n",
        "                index += 1\n",
        "\n",
        "    V = len(vocabulary)\n",
        "    # Initialize embedding matrix\n",
        "    E = np.random.rand(V, embedding_dim)\n",
        "\n",
        "    # Assign embeddings\n",
        "    for word, idx in vocabulary.items():\n",
        "        if word in pretrained_embeddings:\n",
        "            E[idx] = np.array(pretrained_embeddings[word])\n",
        "        else:\n",
        "            E[idx] = np.random.rand(embedding_dim)  # Random initialization\n",
        "\n",
        "    # Define get_word_vector function\n",
        "    def get_word_vector(word):\n",
        "        word = word.lower()\n",
        "        if word in vocabulary:\n",
        "            idx = vocabulary[word]\n",
        "            return E[idx]\n",
        "        else:\n",
        "            return np.zeros(embedding_dim)\n",
        "\n",
        "    return E, vocabulary, get_word_vector\n",
        "\n",
        "# Example usage:\n",
        "corpus = [\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning is amazing\",\n",
        "    \"I love learning new things\"\n",
        "]\n",
        "\n",
        "pretrained_embeddings = {\n",
        "    \"machine\": [0.1, 0.2, 0.3],\n",
        "    \"learning\": [0.2, 0.3, 0.4],\n",
        "    \"amazing\": [0.3, 0.4, 0.5],\n",
        "    \"love\": [0.4, 0.5, 0.6]\n",
        "}\n",
        "\n",
        "embedding_dim = 3\n",
        "\n",
        "E, vocabulary, get_word_vector = create_embedding_matrix_with_pretrained(\n",
        "    corpus, pretrained_embeddings, embedding_dim)\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"Embedding Matrix E:\\n\", E)\n",
        "\n",
        "# Test get_word_vector\n",
        "word = \"machine\"\n",
        "vector = get_word_vector(word)\n",
        "print(f\"Embedding for '{word}':\", vector)\n",
        "\n",
        "word = \"i\"\n",
        "vector = get_word_vector(word)\n",
        "print(f\"Embedding for '{word}':\", vector)  # Randomly initialized\n",
        "\n",
        "word = \"unknown\"\n",
        "vector = get_word_vector(word)\n",
        "print(f\"Embedding for '{word}':\", vector)  # Returns zeros\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ3w2u-Anh0f",
        "outputId": "e8943713-3e41-40e6-870e-125e5ee46122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'i': 0, 'love': 1, 'machine': 2, 'learning': 3, 'is': 4, 'amazing': 5, 'new': 6, 'things': 7}\n",
            "Embedding Matrix E:\n",
            " [[0.2961402  0.11872772 0.31798318]\n",
            " [0.4        0.5        0.6       ]\n",
            " [0.1        0.2        0.3       ]\n",
            " [0.2        0.3        0.4       ]\n",
            " [0.41426299 0.0641475  0.69247212]\n",
            " [0.3        0.4        0.5       ]\n",
            " [0.56660145 0.26538949 0.52324805]\n",
            " [0.09394051 0.5759465  0.9292962 ]]\n",
            "Embedding for 'machine': [0.1 0.2 0.3]\n",
            "Embedding for 'i': [0.2961402  0.11872772 0.31798318]\n",
            "Embedding for 'unknown': [0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6\n",
        "import numpy as np\n",
        "\n",
        "def create_one_hot_encodings(corpus):\n",
        "    # Preprocessing\n",
        "    vocabulary = {}\n",
        "    index = 0\n",
        "    for sentence in corpus:\n",
        "        words = sentence.lower().split()\n",
        "        for word in words:\n",
        "            if word not in vocabulary:\n",
        "                vocabulary[word] = index\n",
        "                index += 1\n",
        "\n",
        "    V = len(vocabulary)\n",
        "    # Initialize one-hot encoding matrix\n",
        "    one_hot_encodings = {}\n",
        "\n",
        "    for word, idx in vocabulary.items():\n",
        "        one_hot_vector = np.zeros(V)\n",
        "        one_hot_vector[idx] = 1\n",
        "        one_hot_encodings[word] = one_hot_vector\n",
        "\n",
        "    return vocabulary, one_hot_encodings\n",
        "\n",
        "# Example usage:\n",
        "corpus = [\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning is amazing\",\n",
        "    \"I love learning new things\"\n",
        "]\n",
        "\n",
        "vocabulary, one_hot_encodings = create_one_hot_encodings(corpus)\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"\\nOne-Hot Encodings:\")\n",
        "for word, one_hot_vector in one_hot_encodings.items():\n",
        "    print(f\"Word: '{word}' - One-Hot Vector: {one_hot_vector}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8alAeeEen_8i",
        "outputId": "4dd79d59-ee92-4f75-e9ec-854aad15b344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'i': 0, 'love': 1, 'machine': 2, 'learning': 3, 'is': 4, 'amazing': 5, 'new': 6, 'things': 7}\n",
            "\n",
            "One-Hot Encodings:\n",
            "Word: 'i' - One-Hot Vector: [1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Word: 'love' - One-Hot Vector: [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Word: 'machine' - One-Hot Vector: [0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Word: 'learning' - One-Hot Vector: [0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Word: 'is' - One-Hot Vector: [0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Word: 'amazing' - One-Hot Vector: [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Word: 'new' - One-Hot Vector: [0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Word: 'things' - One-Hot Vector: [0. 0. 0. 0. 0. 0. 0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7\n",
        "def generate_skip_gram_pairs(sentences, window_size):\n",
        "    # Preprocessing: Build the vocabulary and word indices\n",
        "    vocabulary = {}\n",
        "    index = 0\n",
        "    for sentence in sentences:\n",
        "        words = sentence.lower().split()\n",
        "        for word in words:\n",
        "            if word not in vocabulary:\n",
        "                vocabulary[word] = index\n",
        "                index += 1\n",
        "\n",
        "    # Generate skip-gram training pairs\n",
        "    training_pairs = []\n",
        "    for sentence in sentences:\n",
        "        words = sentence.lower().split()\n",
        "        for i, target_word in enumerate(words):\n",
        "            # Define the context window\n",
        "            start = max(0, i - window_size)\n",
        "            end = min(len(words), i + window_size + 1)\n",
        "            for j in range(start, end):\n",
        "                if i != j:\n",
        "                    context_word = words[j]\n",
        "                    training_pairs.append((target_word, context_word))\n",
        "\n",
        "    return vocabulary, training_pairs\n",
        "\n",
        "# Example usage:\n",
        "sentences = [\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning is amazing\",\n",
        "    \"I love learning new things\"\n",
        "]\n",
        "\n",
        "window_size = 2\n",
        "\n",
        "vocabulary, training_pairs = generate_skip_gram_pairs(sentences, window_size)\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"\\nSkip-Gram Training Pairs:\")\n",
        "for pair in training_pairs:\n",
        "    print(pair)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKugMkBEoQhI",
        "outputId": "4f43bab5-c742-4d35-c023-f0c206105abb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'i': 0, 'love': 1, 'machine': 2, 'learning': 3, 'is': 4, 'amazing': 5, 'new': 6, 'things': 7}\n",
            "\n",
            "Skip-Gram Training Pairs:\n",
            "('i', 'love')\n",
            "('i', 'machine')\n",
            "('love', 'i')\n",
            "('love', 'machine')\n",
            "('love', 'learning')\n",
            "('machine', 'i')\n",
            "('machine', 'love')\n",
            "('machine', 'learning')\n",
            "('learning', 'love')\n",
            "('learning', 'machine')\n",
            "('machine', 'learning')\n",
            "('machine', 'is')\n",
            "('learning', 'machine')\n",
            "('learning', 'is')\n",
            "('learning', 'amazing')\n",
            "('is', 'machine')\n",
            "('is', 'learning')\n",
            "('is', 'amazing')\n",
            "('amazing', 'learning')\n",
            "('amazing', 'is')\n",
            "('i', 'love')\n",
            "('i', 'learning')\n",
            "('love', 'i')\n",
            "('love', 'learning')\n",
            "('love', 'new')\n",
            "('learning', 'i')\n",
            "('learning', 'love')\n",
            "('learning', 'new')\n",
            "('learning', 'things')\n",
            "('new', 'love')\n",
            "('new', 'learning')\n",
            "('new', 'things')\n",
            "('things', 'learning')\n",
            "('things', 'new')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8\n",
        "def generate_cbow_pairs(sentences, window_size):\n",
        "    # Preprocessing: Build the vocabulary and word indices\n",
        "    vocabulary = {}\n",
        "    index = 0\n",
        "    for sentence in sentences:\n",
        "        words = sentence.lower().split()\n",
        "        for word in words:\n",
        "            if word not in vocabulary:\n",
        "                vocabulary[word] = index\n",
        "                index += 1\n",
        "\n",
        "    # Generate CBOW training pairs\n",
        "    training_pairs = []\n",
        "    for sentence in sentences:\n",
        "        words = sentence.lower().split()\n",
        "        for i, target_word in enumerate(words):\n",
        "            # Define the context window\n",
        "            start = max(0, i - window_size)\n",
        "            end = min(len(words), i + window_size + 1)\n",
        "            context_words = []\n",
        "            for j in range(start, end):\n",
        "                if i != j:\n",
        "                    context_words.append(words[j])\n",
        "            if context_words:\n",
        "                training_pairs.append((tuple(context_words), target_word))\n",
        "\n",
        "    return vocabulary, training_pairs\n",
        "\n",
        "# Example usage:\n",
        "sentences = [\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning is amazing\",\n",
        "    \"I love learning new things\"\n",
        "]\n",
        "\n",
        "window_size = 2\n",
        "\n",
        "vocabulary, training_pairs = generate_cbow_pairs(sentences, window_size)\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"\\nCBOW Training Pairs:\")\n",
        "for pair in training_pairs:\n",
        "    print(f\"Context: {pair[0]}, Target: {pair[1]}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wi_BzESZokrA",
        "outputId": "78da1256-0706-4a39-976f-2f8e18844698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'i': 0, 'love': 1, 'machine': 2, 'learning': 3, 'is': 4, 'amazing': 5, 'new': 6, 'things': 7}\n",
            "\n",
            "CBOW Training Pairs:\n",
            "Context: ('love', 'machine'), Target: i\n",
            "Context: ('i', 'machine', 'learning'), Target: love\n",
            "Context: ('i', 'love', 'learning'), Target: machine\n",
            "Context: ('love', 'machine'), Target: learning\n",
            "Context: ('learning', 'is'), Target: machine\n",
            "Context: ('machine', 'is', 'amazing'), Target: learning\n",
            "Context: ('machine', 'learning', 'amazing'), Target: is\n",
            "Context: ('learning', 'is'), Target: amazing\n",
            "Context: ('love', 'learning'), Target: i\n",
            "Context: ('i', 'learning', 'new'), Target: love\n",
            "Context: ('i', 'love', 'new', 'things'), Target: learning\n",
            "Context: ('love', 'learning', 'things'), Target: new\n",
            "Context: ('learning', 'new'), Target: things\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9\n",
        "import numpy as np\n",
        "\n",
        "def rnn_forward(x, Wxh, Whh, Why, bh, by, h0):\n",
        "    h = h0\n",
        "    hs, ys = [], []\n",
        "    for xt in x:\n",
        "        xt = np.array([[xt]])  # Input at time step (column vector)\n",
        "        h = np.tanh(np.dot(Whh, h) + np.dot(Wxh, xt) + bh)  # Hidden state update\n",
        "        y = np.dot(Why, h) + by  # Output at time step\n",
        "        hs.append(h)\n",
        "        ys.append(y)\n",
        "    return ys, hs\n",
        "\n",
        "# Example usage:\n",
        "x = [1, 2, 3]  # Input sequence\n",
        "input_size, hidden_size, output_size = 1, 4, 1  # Sizes for input, hidden, output\n",
        "\n",
        "# Random initialization of weights and biases\n",
        "np.random.seed(0)\n",
        "Wxh = np.random.randn(hidden_size, input_size) * 0.01\n",
        "Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "Why = np.random.randn(output_size, hidden_size) * 0.01\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((output_size, 1))\n",
        "h0 = np.zeros((hidden_size, 1))  # Initial hidden state\n",
        "\n",
        "# Run the RNN\n",
        "ys, hs = rnn_forward(x, Wxh, Whh, Why, bh, by, h0)\n",
        "\n",
        "# Output results\n",
        "print(\"Outputs at each time step:\")\n",
        "for t, y in enumerate(ys):\n",
        "    print(f\"Time step {t+1}: y = {y.flatten()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypdsk5Wpo2jg",
        "outputId": "61e3a46c-26c5-440c-df58-adb0588aa359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outputs at each time step:\n",
            "Time step 1: y = [-0.00050584]\n",
            "Time step 2: y = [-0.00101643]\n",
            "Time step 3: y = [-0.00152624]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10\n",
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))\n",
        "    return exp_x / np.sum(exp_x)\n",
        "\n",
        "def self_attention(X, Wq, Wk, Wv):\n",
        "    # Compute Queries (Q), Keys (K), and Values (V)\n",
        "    Q = np.dot(X, Wq)\n",
        "    K = np.dot(X, Wk)\n",
        "    V = np.dot(X, Wv)\n",
        "\n",
        "\n",
        "    attention_scores = np.dot(Q, K.T)\n",
        "\n",
        "    # Apply softmax to attention scores\n",
        "    attention_weights = softmax(attention_scores)\n",
        "\n",
        "    # Compute final output: Attention weights * V\n",
        "    output = np.dot(attention_weights, V)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(4, 3)\n",
        "Wq = np.random.rand(3,2)\n",
        "Wk = np.random.rand(3,2)\n",
        "Wv = np.random.rand(3,2)\n",
        "\n",
        "output = self_attention(X, Wq, Wk, Wv)\n",
        "\n",
        "print(\"Input Matrix X:\")\n",
        "print(X)\n",
        "print(\"\\nWeight Matrix Wq:\")\n",
        "print(Wq)\n",
        "print(\"\\nWeight Matrix Wk:\")\n",
        "print(Wk)\n",
        "print(\"\\nWeight Matrix Wv:\")\n",
        "print(Wv)\n",
        "print(\"\\nSelf-Attention Output:\")\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1vul3U3pE7d",
        "outputId": "5601fd43-76da-4dbc-bd36-1a5bf58b54d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Matrix X:\n",
            "[[0.5488135  0.71518937 0.60276338]\n",
            " [0.54488318 0.4236548  0.64589411]\n",
            " [0.43758721 0.891773   0.96366276]\n",
            " [0.38344152 0.79172504 0.52889492]]\n",
            "\n",
            "Weight Matrix Wq:\n",
            "[[0.56804456 0.92559664]\n",
            " [0.07103606 0.0871293 ]\n",
            " [0.0202184  0.83261985]]\n",
            "\n",
            "Weight Matrix Wk:\n",
            "[[0.77815675 0.87001215]\n",
            " [0.97861834 0.79915856]\n",
            " [0.46147936 0.78052918]]\n",
            "\n",
            "Weight Matrix Wv:\n",
            "[[0.11827443 0.63992102]\n",
            " [0.14335329 0.94466892]\n",
            " [0.52184832 0.41466194]]\n",
            "\n",
            "Self-Attention Output:\n",
            "[[0.13865641 0.33354634]\n",
            " [0.13573272 0.32654002]\n",
            " [0.18386379 0.44127869]\n",
            " [0.08689064 0.20968136]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LgC_Dkzsdw9b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}